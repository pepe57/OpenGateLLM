{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81d87e1a-014f-43a2-a0a5-703bd158f0f9",
   "metadata": {},
   "source": [
    "# Chat completions\n",
    "\n",
    "> **Warning**\n",
    ">\n",
    "> For the following tutorial, we use [DINUM](https://www.numerique.gouv.fr/) instance of OpenGateLLM, called [Albert API](https://albert.api.etalab.gouv.fr/swagger). If your are not a user of this instance, please refer to the [OpenGateLLM readme](https://github.com/etalab-ia/OpenGateLLM?tab=readme-ov-file#-tutorials--guides) to install and configure your own instance. You need to have a text-generation or image-text-to-text model to run this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df030ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU openai\n",
    "\n",
    "import os\n",
    "\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341f5bbe",
   "metadata": {},
   "source": [
    "First, setup your API key in the environment variable `ALBERT_API_KEY` for OpenAI client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a18feb-e58b-4fb3-809e-045a81bec9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://albert.api.etalab.gouv.fr/v1\"\n",
    "api_key = os.getenv(\"ALBERT_API_KEY\")\n",
    "client = OpenAI(base_url=base_url, api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e82c895",
   "metadata": {},
   "source": [
    "Retrieve the list of available models with `/v1/models` endpoint for chat completions. Theses models have the type `text-generation` or `image-text-to-text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4226933e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat model found: albert-small\n"
     ]
    }
   ],
   "source": [
    "models = client.models.list().data\n",
    "\n",
    "model = [model for model in models if model.type in [\"text-generation\", \"image-text-to-text\"]][0].id\n",
    "print(f\"Chat model found: {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56c936d",
   "metadata": {},
   "source": [
    "### Unstreamed chat\n",
    "\n",
    "Run a unstreamed chat with `/v1/chat/completions` endpoint by OpenAI client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f700cab-e53f-4a4c-8cbc-be9bdf96a7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat result: Hello there! Unfortunately, I'm not Albert Einstein, but I'll do my best to help you with any questions or topics you'd like to discuss. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    \"model\": model,\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Hi Albert !\"}],\n",
    "    \"stream\": False,\n",
    "    \"n\": 1,\n",
    "}\n",
    "\n",
    "response = client.chat.completions.create(**data)\n",
    "print(f\"Chat result: {response.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8fcb6b",
   "metadata": {},
   "source": [
    "### Streamed chat\n",
    "\n",
    "Run a streamed chat with `/v1/chat/completions` endpoint by OpenAI client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f7753b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat result:\n",
      "\n",
      "token: Not\n",
      "token:  much\n",
      "token: .\n",
      "token:  Just\n",
      "token:  here\n",
      "token:  to\n",
      "token:  help\n",
      "token:  answer\n",
      "token:  any\n",
      "token:  questions\n",
      "token:  or\n",
      "token:  chat\n",
      "token:  with\n",
      "token:  you\n",
      "token: .\n",
      "token:  What\n",
      "token: 's\n",
      "token:  on\n",
      "token:  your\n",
      "token:  mind\n",
      "token: ?\n",
      "token: "
     ]
    }
   ],
   "source": [
    "# streamed chat\n",
    "data = {\n",
    "    \"model\": model,\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What's up Albert ?\"}],\n",
    "    \"stream\": True,\n",
    "    \"n\": 1,\n",
    "}\n",
    "\n",
    "response = client.chat.completions.create(**data)\n",
    "print(\"Chat result:\")\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].finish_reason is not None:\n",
    "        break\n",
    "    print(chunk.choices[0].delta.content, end=\"\\ntoken: \", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
